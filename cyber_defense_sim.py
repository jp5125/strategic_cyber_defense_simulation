# -*- coding: utf-8 -*-
"""cyber_defense_sim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuD8dIN79AHVvJbFDM5VMLPfoIPbGlYL
"""

#Initial utility methods and model parameters

import numpy as np
import pandas as pd
from enum import IntEnum


#Utility methods

#clip01 keeps all continuous variables in the simulation bounded between [0,1]
def clip01(x):
  return np.clip(x, 0.0, 1.0)

# Enums for ensuring clean and explicit model events
class Action(IntEnum):
  PASSIVE = 0 # NIST CSF 2.0 Functions Identify + Protect
  ACTIVE = 1 # NIST CSF 2.0 Functions Detect + Respond
  RECOVER = 2 #Recover function

class AttackTarget(IntEnum):
  NONE = 0
  IT = 1
  OT = 2

class Intensity(IntEnum):
  NONE = 0
  LOW = 1
  HIGH = 2

# Model Parameters
Parameters = pd.Series({
    #Simulation Control
    "T" : 500,
    'Seed': 1,

    #Governance
    "G" : 0.6,

    #Initial Defender State variables
    "it_vuln_init" : 0.6,
    "ot_vuln_init" : 0.7,
    "id_cap_init" : 0.2,
    'it_comp_init' : 0,
    'ot_comp_init' : 0,
    'downtime_init' : 0.0,
    'phys_damage_init' : 0.0,
    'outage_init': 0.0,

    #Attacker event process
    'p_attack': 0.35,
    'p_ot_given_attack_base' : 0.35,
    'p_ot_bonus_if_it_comp' : 0.20,
    'p_ot_bonus_if_ot_high_vuln' : 0.20,
    'ot_high_vuln_threshold': 0.7,

    #Attacker success parameters, as of 1/16/26 these are placeholder values
    'base_success_mult': 1.0,
    'high_success_bonus': 0.25, #high intensity attack have an additive bonus

    #Attack intensity deterrence via identification
    'p_high_base' : 0.50,
    'k_deterrence' : 2.0,

    # Delta parameters for defender actions
    "delta_it_vuln": 0.04,
    "delta_ot_vuln": 0.02,
    "delta_id_cap": 0.03,

    #defender detection and containment parameters (placeholders for now, 1/20/26)
    "p_detect_base": 0.10, #baseline detection probability
    "p_contain_base": 0.20, #baseline containment probability

    #Changes in defender stats based on previous turn actions (PASSIVE, ACTIVE, RECOVER)
    "delta_detect": 0.25,
    "delta_contain": 0.25,
    "active_damage_reduction": 0.35,

    "delta_recover_clear": 0.30,
    "delta_downtime_reduction": 0.40,

    #Damage and downtime dynamics (placeholders until we tune parameter values)
    'base_damage' : 0.02, #per timestep damage done to defender while OT is compromised
    'high_damage_multiplier' : 3.0, #multiplication factor for when attacks are high intensity
    'damage_persistence': 1.0, #1.0 means no decay, if we decrease below 1.0 systems damage will decay over time

    'downtime_comp_cost': 0.05, #cost of downtime increases the longer a system remains compromised
    'downtime_damage_cost' : 0.02, #additional downtime increases per step unit damage
    'downtime_decay': 0.0, #can add a natural recovery function by making downtime_decay > 0.0

    #Recovery probabilities
    'p_recover_clear_base' : 0.10, #chance recovery clears compromise status
    'damage_recover_decay' : 0.02 #fraction of damage removed under RECOVER action
})

#Reproducible randomness
rng = np.random.default_rng(int(Parameters["Seed"]))

print(Parameters)

#Simulation State Code

#Initialize state as a Pandas series
def make_initial_state(Parameters):
  return pd.Series({
    'it_vuln' : clip01(Parameters['it_vuln_init']),
    'ot_vuln' : clip01(Parameters['ot_vuln_init']),
    'id_cap' : clip01(Parameters['id_cap_init']),

    #defender compromised treated as boolean, flags [0,1] indicate whether defender IT or OT is compromised
    "it_comp" : int(Parameters['it_comp_init']),
    "ot_comp" : int(Parameters['ot_comp_init']),
    "downtime" : float(Parameters['downtime_init']),
    "phys_damage" : float(Parameters['phys_damage_init']),
    "outage" : float(Parameters['outage_init']),
    })

State = make_initial_state(Parameters)

#Simulation time + log storage data structures
t = 0
rows = []

def gov_mult(Parameters):
  #baseline government multiplier = 0.5 + 0.5 * G
  return 0.5 + 0.5 * clip01(Parameters['G'])


def snapshot_state(Parameters, State, t):
  #returns a dictionary of the pre-action state we wish to record in the log
  return{
      't' : t,
      'G' : float(clip01(Parameters['G'])),
      'gov_mult': float(gov_mult(Parameters)),

      'it_vuln' : float(State['it_vuln']),
      'ot_vuln' : float(State['ot_vuln']),
      'id_cap' : float(State['id_cap']),

      'it_comp' : int(State['it_comp']),
      'ot_comp' : int(State['ot_comp']),
      'downtime' : float(State['downtime']),
      'phys_damage' : float(State['phys_damage']),
      'outage' : float(State['outage'])
  }


def sim_step(Parameters, State, rng, t, rows):
  """
  Simulation Loop event ordering is as follows:
  1. First, the defender chooses an action (PASSIVE, ACTIVE, or RECOVER) to play for the current timestep
  2. The attacker than chooses whether or not it will attack and at what intensity
  3. We resolve the attack phase and determine if there is any damage to the defenders IT or OT layers
  4. Defender Detection/Containment procedures attempt to detect damage and if detected, mitigate damage
  5. OT damage accumulates if ot_comp remains
  6. downtime of defender infrastructure (represented by ot_layer) updates
  7. If in RECOVER, an additional step may clear compromise and reduce damage missed by the detection/containment step
  8. if policy = qlearn, run update_step method for q-learning
  """
  pre = snapshot_state(Parameters, State, t)

  policy = Parameters.get('defender_policy', 'always_passive')
  s_pre = discretize_state(Parameters, State) if policy == 'qlearn_v1' else None

  #Placeholder policy: always PASSIVE for now (1/6/26)
  action = choose_action(Parameters, State, rng, t)

  #Defender action effects
  B = apply_defender_action(Parameters, State, action)

  #Attacker strategy determination
  attack_target, intensity = sample_attacker_event(Parameters, State, rng)

  #Attack resolution
  p_success, attack_success = resolve_attack(Parameters, State, rng, attack_target, intensity)

  #Defender detection and containment step
  dc = detection_and_containment_step(Parameters, State, rng, B)

  #compromise status after detection/containment, but before recovery
  it_comp_post_dc = int(State['it_comp'])
  ot_comp_post_dc = int(State['ot_comp'])

  #damage step
  damage_step = ot_physical_damage_step(Parameters, State, intensity, B)

  #downtime
  downtime_step = downtime_update_step(Parameters, State, B, action)

  #recovery action step (if recovery action is chosen)
  recovery = recovery_resolution_step(Parameters, State, rng, B, action)

  #compromise state after recovery step
  it_comp_end = int(State['it_comp'])
  ot_comp_end = int(State['ot_comp'])

  #system outage state at end of timestep
  outage_status = outage_update_step(Parameters, State)
  outage_end = float(State['outage'])

  #additional learning step for Qlearn policy
  rl_reward = 0.0
  if policy == 'qlearn_v1':
    rl_reward = qlearn_update_step(Parameters, State, Q_AGENT, s_pre = s_pre, action = action, damage_step = damage_step, it_comp_end = it_comp_end, ot_comp_end = ot_comp_end)


  #Log row for simulation data collection
  row = dict(pre)
  row.update({
     'action' : int(action),
     'action_name' : action.name,

     #log boosts to active defense values
     'detect_boost': float(B["detect_boost"]),
     'contain_boost': float(B["contain_boost"]),
     'recover_clear_boost': float(B['recover_clear_boost']),
     'downtime_reduction_boost': float(B['downtime_reduction_boost']),
     'active_damage_reduction': float(B['active_damage_reduction']),

     #attacker event
     'attack': int(attack_target),
     'attack_name': attack_target.name,
     'intensity': int(intensity),
     'intensity_name': intensity.name,
     'p_high': float(p_high_given_idcap(Parameters, State)),

     #attack outcome values
     "p_success": float(p_success),
     "attack_success": int(attack_success),

      # compromise status after detect/containment (pre-recovery)
     "it_comp_post_dc": it_comp_post_dc,
     "ot_comp_post_dc": ot_comp_post_dc,

     # compromise status at end of timestep (post-recovery)
     "it_comp_end": it_comp_end,
     "ot_comp_end": ot_comp_end,

     #Next state values
     'it_vuln_next' : float(State['it_vuln']),
     'ot_vuln_next' : float(State['ot_vuln']),
     'id_cap_next' : float(State['id_cap']),

     #damage and recovery values
     'damage_step': float(damage_step),
     'phys_damage_next': float(State['phys_damage']),
     'downtime_step': float(downtime_step),
     'downtime_next': float(State['downtime']),
     'outage_status': outage_status,
     'outage_next': outage_end,

     #rl values
     'rl_reward' : rl_reward,
     'q_size': len(Q_AGENT.Q) if policy == 'qlearn_v1' else np.nan

    })

  row.update(dc)
  row.update(recovery)

  rows.append(row)
  return t + 1 #advance time


def run_sim(Parameters, State, rng):
  rows_local = []
  t_local = 0

  for _ in range(int(Parameters['T'])):
    t_local = sim_step(Parameters, State, rng, t_local, rows_local)

  return pd.DataFrame(rows_local)

#Now, lets add some defense-effect parameters

def init_boosts():
    return pd.Series({
        "detect_boost": 0.0,
        "contain_boost": 0.0,
        "recover_clear_boost": 0.0,
        "downtime_reduction_boost": 0.0,
        "active_damage_reduction": 0.0
    })

def apply_defender_action(Parameters, State, action):
    gm = gov_mult(Parameters)
    B = init_boosts()

    if action == Action.PASSIVE:
        State["it_vuln"] = clip01(State["it_vuln"] - gm * Parameters["delta_it_vuln"])
        State["ot_vuln"] = clip01(State["ot_vuln"] - gm * Parameters["delta_ot_vuln"])
        State["id_cap"]  = clip01(State["id_cap"]  + gm * Parameters["delta_id_cap"])

    elif action == Action.ACTIVE:
        B["detect_boost"] = gm * Parameters["delta_detect"]
        B["contain_boost"] = gm * Parameters["delta_contain"]
        B["active_damage_reduction"] = clip01(gm * Parameters["active_damage_reduction"])

    elif action == Action.RECOVER:
        B["recover_clear_boost"] = gm * Parameters["delta_recover_clear"]
        B["downtime_reduction_boost"] = clip01(gm * Parameters["delta_downtime_reduction"])

    else:
        raise ValueError(f"Invalid action: {action}")

    return B

""" Implementation of the attacker process"""

def p_high_given_idcap(Parameters, State):
  """calculates the probability that the attacker will use a high intensity attack as a function of the identifying capabilities of the defender entity"""
  # P(HIGH) = p_high_base * exp(-k_deterrence * id_cap)

  return clip01(Parameters['p_high_base'] * np.exp(-Parameters['k_deterrence'] * State['id_cap']))

def sample_attacker_event(Parameters, State, rng):
  """
  After calculating the probability the attacker uses a high-intensity attack this timestep,
  We determine based on this probability if the attacker attacks this turn and if so,
  1. Is the target IT or OT infrastructure layer?
  2. Is the attack high or low intensity?
  """

  if rng.random() > Parameters['p_attack']:
    return(AttackTarget.NONE, Intensity.NONE)

  p_ot = Parameters['p_ot_given_attack_base']
  it_comp_bonus = Parameters['p_ot_bonus_if_it_comp']
  ot_vuln_bonus = Parameters['p_ot_bonus_if_ot_high_vuln']

  if State['it_comp'] == 1:
    if State['ot_vuln'] >= Parameters['ot_high_vuln_threshold']:
      p_ot = p_ot + it_comp_bonus + ot_vuln_bonus
    else:
      p_ot = p_ot + it_comp_bonus
  else:
    if State['ot_vuln'] >= Parameters['ot_high_vuln_threshold']:
      p_ot = p_ot + ot_vuln_bonus
    else:
      p_ot = p_ot

  p_ot = clip01(p_ot)
  target = AttackTarget.OT if rng.random() < p_ot else AttackTarget.IT


  p_high = p_high_given_idcap(Parameters, State)
  intensity = Intensity.HIGH if rng.random() < p_high else Intensity.LOW

  return target, intensity

#Attacker resolution logic implementation

def attack_success_probability(Parameters, State, attack_target, intensity):
  """
  Once all of the details regarding attacker target and intensity are determined for the current timestep,
  we use this function to determine if the attack is successful.
  """

  #Determine if the defender's relevant vulnerability level for success calculation uses it_vuln or ot_vuln
  if attack_target == AttackTarget.IT:
    vuln = State['it_vuln']
  elif attack_target == AttackTarget.OT:
    vuln = State['ot_vuln']
  else:
    return 0.0

  #Now, we need to calculate the success probability scaled by vulnerability level
  p_success = Parameters['base_success_mult'] * vuln

  #add intensity effect if HIGH intensity attack is happening
  if intensity == Intensity.HIGH:
    p_success += Parameters["high_success_bonus"]

  return float(clip01(p_success))


def resolve_attack(Parameters, State, rng, attack_target, intensity):
  """
  Now we compute the resolution of the attack phase using the
  attack success probability function. We will return the calculated
  probability of success as well as the attack outcome to be logged.
  """

  if attack_target == AttackTarget.NONE:
    return 0.0 , 0

  #save the calculated probability of success as a variable
  p_success = attack_success_probability(Parameters, State, attack_target, intensity)
  success = 1 if rng.random() < p_success else 0

  if success == 1:
    if attack_target == AttackTarget.IT:
      State["it_comp"] = 1
    elif attack_target == AttackTarget.OT:
      State["ot_comp"] = 1

  return p_success, success

def detect_and_contain_one(Parameters, State, rng, comp_key, detect_boost, contain_boost):
  """If the state of the defender's infrastructure this time step is compromised we do the following:
     1. the defender detects the compromise with p_detect = p_detect_base + detect_boost
     2. if detection is successful, we then determine if the damage can be contained with P-contain = p_contain_base + contain_boost
     3. if contained, set State[comp_key] == 0
     *** comp key can be either it_comp or ot_comp

     Returns: (detected_flag, contained_flag)
  """

  if int(State[comp_key]) != 1:
    return 0,0

  p_detect = clip01(Parameters['p_detect_base'] + float(detect_boost))
  detected = 1 if rng.random() < p_detect else 0

  if detected == 0:
    return 0, 0

  p_contain = clip01(Parameters['p_contain_base'] + float(contain_boost))
  contained = 1 if rng.random() < p_contain else 0

  if contained == 1:
    State[comp_key] = 0

  return detected, contained

def detection_and_containment_step(Parameters, State, rng, B):
  """This function applies detection and containment logic for IT and OT compromises.
    It returns a dict of outcomes for logging and data analysis
  """

  it_detected, it_contained = detect_and_contain_one(Parameters, State, rng, comp_key = 'it_comp', detect_boost = B['detect_boost'], contain_boost = B['contain_boost'])

  ot_detected, ot_contained = detect_and_contain_one(Parameters, State, rng, comp_key = 'ot_comp', detect_boost = B['detect_boost'], contain_boost = B['contain_boost'])

  return{
      "it_detected" : it_detected,
      "it_contained": it_contained,
      "ot_detected": ot_detected,
      "ot_contained": ot_contained,
      "it_comp_post": int(State['it_comp']), #it_compromise status after detection and containment is run
      "ot_comp_post": int(State['ot_comp']) #ot_compromise status after detetcion and containment is run this time step
  }

#This set of functions sets physical damage to the ot_layer from attacks and provides recovery resolution function for the defender


outage_defaults = {
    'outage_decay' : 0.60,
    'outage_comp_cost' : 0.4,
    'outage_damage_cost' : 0.2
}

for k,v in outage_defaults.items():
  if k not in Parameters.index:
    Parameters[k] = v


def ot_physical_damage_step(Parameters, State, intensity, B):
  """If the OT layer is compromised, we add physical damage to the defender infrastructure
  1. start with base damage per step for low intensity atacks
  2. add multiplication factor for high intensity attacks
  3. reduce damage dealt based on the defenders' ACTIVE damage reduction boost
  *** returns damage for logging
  """

  if int(State['ot_comp']) != 1:
    return 0.0

  damage = float(Parameters['base_damage'])
  if intensity == Intensity.HIGH:
    damage *= float(Parameters['high_damage_multiplier'])

  #active defense boosts reduce the damage done to defender
  damage *= (1.0 - float(B['active_damage_reduction']))

  #apply damage to the defender's systems and ensure this value is not negative
  State["phys_damage"] = max(0.0, float(State['phys_damage']) + damage)
  return float(damage)

def downtime_update_step(Parameters, State, B, action):
  """Updates the downtime the defender has experienced thus far in the sim
  1. increases downtime value if a compromise is present
  2. further increases with accumulated physical damage
  3. RECOVER reduces downtime via boosts which are applied in this function
  """

  comp_present = int(int(State['it_comp']) == 1 or int(State['ot_comp']) == 1)

  dt_counter = 0.0
  dt_counter += float(Parameters['downtime_comp_cost'] * comp_present)
  dt_counter += float(Parameters['downtime_damage_cost']) * float(State['phys_damage'])

  #optional natural decay of downtime (currently set to 0)
  dt = float(State['downtime']) + dt_counter
  dt = max(0.0, dt - float(Parameters.get('downtime_decay', 0.0)))

  #If recover is the defenders chosen action this turn, apply a downtime reduction boost
  if action == Action.RECOVER:
    dt = max(0.0, dt * (1.0 - float(B["downtime_reduction_boost"])))

  State['downtime'] = float(dt)
  return float(dt_counter)

def outage_update_step(Parameters, State):
  comp_present = int(int(State['it_comp']) == 1 or int(State['ot_comp']) == 1)

  out = 0.0
  out += float(Parameters.get('outage_comp_cost', 0.40)) * float(comp_present)
  out += float(Parameters.get('outage_damage_cost', 0.20)) * float(State['phys_damage'])

  decay = float(Parameters.get('outage_decay', 0.60))
  prev = float(State['outage'])

  new_outage = (1.0 - decay) * prev + out
  State['outage'] = float(clip01(new_outage))

  return float(out)

def recovery_resolution_step(Parameters, State, rng, B, action):
  """If RECOVER is the chosen action of the defender:
  1. probabilistically clear IT/OT compromise even if undetected
  2. optionally, reduce accumulated damage a bit
  """

  out = {
      "recovery_it_cleared": 0,
      'recovery_ot_cleared': 0,
      'damage_reduction': 0.0
  }

  if action != Action.RECOVER:
    return out

  p_clear = clip01(float(Parameters['p_recover_clear_base']) + float(B['recover_clear_boost']))

  if int(State['it_comp']) == 1 and (rng.random() < p_clear):
    State['it_comp'] = 0
    out['recovery_it_cleared'] = 1

  if int(State['ot_comp']) == 1 and (rng.random() < p_clear):
    State['ot_comp'] = 0
    out['recovery_ot_cleared'] = 1

  #logic for implementing an optional modest damage reduction under the RECOVER action
  frac = clip01(float(Parameters.get('damage_recover_decay', 0.0)))
  if frac > 0:
    before = float(State['phys_damage'])
    after = max(0.0, before * (1.0 - frac))
    State['phys_damage'] = after
    out['damage_reduction'] = float(before - after)

  return out

#defender action decision logic

"""
This is the logic the defender will initially use to make decisions about which policy decisions to adopt at any given timestep.
These are currently hard-coded based on my best guesses at a somewhat coherent defensive strategy, however this is mainly a placeholder for the implementation of
some kind of RL algorithm which will be inplemnted later to help optimize defensive strategy decision making.
"""

#First, we need to define thresholds for defender decisions regarding which action to take in a timestep
policy_defaults = {
    'defender_policy': 'threshold_v1',  # options currently implemented: 'always_passive', 'threshold_v1', 'random', 'qlearn_v1'
    'id_cap_min_threshold': 0.30,      # if id_cap is below this, favor ACTIVE
    'phys_damage_threshold': 0.50,     # if damage is above this, favor RECOVER
    'outage_high_threshold': 0.60    # if downtime is above this, favor RECOVER
}

# just a lil logic to ensure the policy default values are added to the Parameter list
for k, v in policy_defaults.items():
  if k not in Parameters.index:
    Parameters[k] = v

#Now we can implement the logic for the defender to actually choose an action

def choose_action(Parameters, State, rng, t):
  """
  Function that uses defender policy to determine which action the defender will choose each time step. There are currently three policies we can have the defender implement:
  1. always_passive: the current baseline/placeholder policy in which the defender just plays PASSIVE no matter what
  2. random: a policy in which the defender uses a uniform, random dist. to pick the three actions (PASSIVE, ACTIVE, RECOVER) at each time step.
  3. threshold_v1: policy which uses a simple heuristic to determine action selection based on parameter thresholds.
  """

  policy = Parameters.get('defender_policy', 'always_passive')

  if policy == 'always_passive':
    return Action.PASSIVE

  if policy == 'random':
    return Action(int(rng.integers(0, 3))) #if 0: Aaction.PASSIVE, if 1: Action.ACTIVE, if 2: Action.RECOVER

  if policy == 'threshold_v1':
    it_comp = int(State['it_comp'])
    ot_comp = int(State['ot_comp'])
    id_cap = float(State['id_cap'])
    phys_damage = float(State['phys_damage'])
    outage = float(State['outage'])

    id_low = float(Parameters.get('id_cap_min_threshold', 0.30))
    dmg_high = float(Parameters.get("phys_damage_threshold", 0.50))
    outage_high = float(Parameters.get('outage_high_threshold', 0.60))

    #Priority 1: if OT is compromised from previous timestep attacks, RECOVER
    if ot_comp == 1:
      return Action.RECOVER

    # Priority 2, if infrastructure has high physical damage or experiences significant downtime, RECOVER
    if phys_damage >= dmg_high or outage >= outage_high:
      return Action.RECOVER

    #Priority 3: if IT layer is compromised, implement ACTIVE response action
    if it_comp == 1:
      return Action.ACTIVE

    #Priority 4: If ability to identify attacker is low, use PASSIVE action to improve capabilities
    if id_cap < id_low:
      return Action.PASSIVE

    # Otherwise, invest in long-term defensive assets
    return Action.PASSIVE

  if policy == 'qlearn_v1':
    s = discretize_state(Parameters, State)
    action = Q_AGENT.select_action(s, float(Parameters['rl_epsilon']), rng)
    return Action(action)

  raise ValueError(f"Unknown defender_policy: {policy}")

"""
Attempt to construct framework to allow for a temporal difference learning implemntation of a q-learning RL algorithm for our defender agent:
This allows our defender agent to improve at choosing actions over time, rather than following one of our previously prescribed heuristic policies like
'always_passive', 'random' or 'threshold_v1'
"""

rl_defaults = {
    #hyperparameters for agent learning, adjustable during parameter sweeps
    'rl_alpha': 0.15,
    'rl_gamma': 0.95,
    'rl_epsilon': 0.10,

    #discretization values, this step allows for us to use the tabular q-learning technique with continuous data
    'rl_id_cap_lo' : 0.33,
    'rl_id_cap_high': 0.66,
    'rl_damage_lo': 0.25,
    'rl_damage_high': 0.75,
    'rl_outage_lo': 0.25,
    'rl_outage_high': 0.60,

    #reward weights, specifically measured in an actions ability to minimize loss per/step per action (reward = -loss)
    'rl_w_damage_step': 5.0,  # physical damage to infrastructure highly penalizes reward return
    'rl_w_outage': 2.0,       # penalty for interruption to infrastructure services
    'rl_w_ot_comp': 1.0,      # penalty for OT compromise
    'rl_w_it_comp': 0.25,     # small IT compromise penalty, since this does not directly influence infrastructure only allows access for further damage

    'rl_learn' : 1,     # 1 = training mode, 0 = evaluation mode (stops updating)

    #added small costs to certain action because I found under certain parameter values the agent would continuously recover
    'rl_cost_active': 0.05,
    'rl_cost_recover': 0.10,
}

for k,v in rl_defaults.items():
  if k not in Parameters.index:
    Parameters[k] = v


def rl_bin(x, lo, high):
  """
  Function that helps discretize state values based on discretization thresholds established in rl_defaults
  0 = little to no damage/compromise to the defender systems
  1 = signifcant damage/compromise to the defender systems
  2 = catastrophic damage/compromise to the defender systems
  """
  if x < lo : return 0
  if x < high: return 1
  return 2

def discretize_state(Parameters, State):
  """
  Discretized State Tuple:
  (it_comp = 2, ot_comp = 2, id_cap_bin = 3, damage_bin = 3, outage_bin =3)
  2 x 2 x 3 x 3 x 3 = 108 possible states in which our defender agent needs learn to make action decisions in
  """

  it_c = int(State['it_comp'])
  ot_c = int(State['ot_comp'])

  id_c_discrete = rl_bin(float(State['id_cap']), float(Parameters['rl_id_cap_lo']), float(Parameters['rl_id_cap_high']))
  damage_discrete = rl_bin(float(State['phys_damage']), float(Parameters['rl_damage_lo']), float(Parameters['rl_damage_high']))
  outage_discrete = rl_bin(float(State['outage']), float(Parameters['rl_outage_lo']), float(Parameters['rl_outage_high']))

  return(it_c, ot_c, id_c_discrete, damage_discrete, outage_discrete)

class QLearner:
  """
  Class blueprint for an agent who implements Qlearn policy
  """
  def __init__(self, n_actions = 3):
    self.n_actions = n_actions
    self.Q = {} #array that tracks the defender states for the Q-table

  #if a one of the 108 state tuples isnt in the q-table yet, create a new row for that particular permutation of state variable values and add an array of 0s into the row
  def row(self, s):
    if s not in self.Q:
      self.Q[s] = np.zeros(self.n_actions, dtype = float)
    return self.Q[s]

  #needed to add this since all states are not being touched during training, thus when select_action calls row(), it creates additional states-value pairs during evaluation
  def qvals(self,s):
    return self.Q.get(s, np.zeros(self.n_actions, dtype = float))

  #function to choose whether agent will either explore by randomly selecting a strategy with p = epsilon, or exploit the current best action choice with p = 1 - epsilon
  def select_action(self, s, epsilon, rng):
    if rng.random() < epsilon:
      return int(rng.integers(0, self.n_actions))

    q = self.qvals(s)
    best_actions = np.flatnonzero(q == q.max())
    return int(rng.choice(best_actions))

  #update the q-value of action under specific state tuple, essentially the Bellman equation
  def update(self, s, a, r, s_next, alpha, gamma):
    q = self.row(s)
    q_next = self.row(s_next)
    td_target = float(r) + float(gamma) * float(np.max(q_next)) # td_target = reward value at the current step plus discounted reward value at next step
    q[a] = q[a] + float(alpha) * (td_target - q[a])


Q_AGENT = QLearner(n_actions = 3)

#enure learning from previous runs is erased for new runs
def reset_qlearner():
  global Q_AGENT
  Q_AGENT = QLearner(n_actions = 3)


# reward calculation, in terms of minimizing loss, for the QAgent
def rl_step_reward(Parameters, damage_step, outage_next, it_comp_end, ot_comp_end, action):
  loss = 0.0
  loss += float(Parameters['rl_w_damage_step']) * float(damage_step)
  loss += float(Parameters['rl_w_outage']) * float(outage_next)
  loss += float(Parameters['rl_w_it_comp']) * float(it_comp_end)
  loss += float(Parameters['rl_w_ot_comp']) * float(ot_comp_end)

  #reduce reward for step by action cost parameter
  cost = 0.0
  if action == Action.ACTIVE:
    cost += float(Parameters.get('rl_cost_active', 0.0))
  if action == Action.RECOVER:
    cost += float(Parameters.get('rl_cost_recover', 0.0))

  return -(float(loss) + float(cost))

#
def qlearn_update_step(Parameters, State, Q_AGENT, s_pre, action, damage_step, it_comp_end, ot_comp_end):
  outage_next = float(State.get('outage', 0.0))
  r = rl_step_reward(Parameters, damage_step, outage_next, it_comp_end, ot_comp_end, action)

  s_post = discretize_state(Parameters, State)

  #ensure learning only occurs during training runs
  if int(Parameters.get('rl_learn', 1)) == 1:
    Q_AGENT.update(s_pre, int(action), r, s_post, alpha = float(Parameters['rl_alpha']), gamma = float(Parameters['rl_gamma']))

  return float(r)

def run_one(Parameters, seed, policy, T, learn=None, epsilon=None):
  P = Parameters.copy()
  P['Seed'] = int(seed)
  P['T'] = int(T)
  P['defender_policy'] = policy
  if learn is not None:
    P['rl_learn'] = int(learn)
  if epsilon is not None:
    P['rl_epsilon'] = float(epsilon)

  local_rng = np.random.default_rng(int(P['Seed']))
  S0 = make_initial_state(P)
  return run_sim(P, S0, local_rng)

def summarize_run(df):
  # general summaries
  out = {}
  out['mean_reward'] = float(df['rl_reward'].mean()) if 'rl_reward' in df.columns else np.nan
  out['mean_outage'] = float(df['outage_next'].mean())
  out['mean_damage_step'] = float(df['damage_step'].mean())
  out['time_it_comp'] = float(df['it_comp_end'].mean())
  out['time_ot_comp'] = float(df['ot_comp_end'].mean())
  out['action_freq'] = (df['action_name'].value_counts(normalize=True)).to_dict()
  out['q_size_end'] = float(df['q_size'].iloc[-1]) if 'q_size' in df.columns and not df['q_size'].isna().all() else np.nan
  return out

def rolling_action_freq(df, window=500):
  # log for frequency of actions over time
  a = df['action_name']
  idx = np.arange(len(df))
  buckets = (idx // window)
  return (df.assign(bucket=buckets)
            .groupby(['bucket','action_name'])
            .size()
            .groupby(level=0)
            .apply(lambda s: (s / s.sum()))
            .unstack(fill_value=0.0))


# test to ensure learning is working, compare to other heuristic policies previously implemented

Parameters['T'] = 20000

# training stage (p(sigma) = explore, 1 - p(sigma) = exploit)
reset_qlearner()
train_df = run_one(Parameters, seed=1, policy='qlearn_v1', T=20000, learn=1, epsilon=float(Parameters['rl_epsilon']))

# evaluation of qlearn policy after training (greedy, no learning)
eval_q_df = run_one(Parameters, seed=2, policy='qlearn_v1', T=5000, learn=0, epsilon=0.0)

# herustic policy runs for comparison to qlearning policy
eval_thr_df = run_one(Parameters, seed=2, policy='threshold_v1', T=5000)
eval_pas_df = run_one(Parameters, seed=2, policy='always_passive', T=5000)
eval_rnd_df = run_one(Parameters, seed=2, policy='random', T=5000)

train_summary = summarize_run(df = train_df)
eval_summary = {
  'qlearn_greedy': summarize_run(eval_q_df),
  'threshold_v1': summarize_run(eval_thr_df),
  'always_passive': summarize_run(eval_pas_df),
  'random': summarize_run(eval_rnd_df),
}

print("TRAIN SUMMARY:", train_summary)
print("EVAL SUMMARY:", eval_summary)

# Learning diagnostics for logging
train_df['reward_roll'] = train_df['rl_reward'].rolling(500).mean()
train_action_mix = rolling_action_freq(train_df, window=500)
eval_action_mix_q = rolling_action_freq(eval_q_df, window=250)

print("Q size end (train):", train_summary['q_size_end'])
print("Train action mix by window (head):")
print(train_action_mix.head())
print("Eval action mix (qlearn greedy) by window (head):")
print(eval_action_mix_q.head())

#Debugging test to make sure Dataframe is produced as intended
df_test = run_sim(Parameters, State.copy(), rng)
df_test.head(10)

#Test of updated functionality, action == pasive should slowly reduce vulnerability and increase id_capability over time (1/6/26)
test2 = run_sim(Parameters, make_initial_state(Parameters), rng)
test2[['t', 'it_vuln', 'it_vuln_next', 'ot_vuln', 'ot_vuln_next', 'id_cap', 'id_cap_next']].head(10)

#Test of attacker functionality implemented on (1/15/26)
test3 = run_sim(Parameters, make_initial_state(Parameters), rng)
test3[['t', 'id_cap', 'attack_name', 'intensity_name', 'p_high']].head(100)

#Test of attack resolution functionality implemented on (1/16/26)
test4 = run_sim(Parameters, make_initial_state(Parameters), rng)
test4[['t', 'attack_name', 'intensity_name', 'p_success', 'attack_success', 'it_comp_end', 'ot_comp_end']].head(30)

#test attack resolution outputs
test5 = run_sim(Parameters, make_initial_state(Parameters), rng)
test5 = test5[['it_comp', 'it_detected', 'it_contained', 'it_comp_post', 'ot_comp', "ot_detected", 'ot_contained', 'ot_comp_post']].head(30)

#test attack damage and recovery outputs
test6 = run_sim(Parameters, make_initial_state(Parameters), rng)
test6 = test6[['t', 'attack_name', 'ot_comp_end', 'damage_step', 'phys_damage_next', 'downtime_step', 'downtime_next']].head(50)

#test defender action choice logic implementation, threshold_v1
Parameters['defender_policy'] = 'threshold_v1'
State['id_cap'] = 0.35
test_policy1 = run_sim(Parameters, make_initial_state(Parameters), rng)

print(test_policy1.head(50))
print(test_policy1["action_name"].value_counts())

test6

